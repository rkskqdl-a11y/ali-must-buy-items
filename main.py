import os
import time
import random
import hmac
import hashlib
import requests
import json
from datetime import datetime

# 1. í™˜ê²½ ë³€ìˆ˜ ë° ì„¤ì •
ALI_APP_KEY = os.environ.get("ALI_APP_KEY", "").strip()
ALI_SECRET = os.environ.get("ALI_SECRET", "").strip()
ALI_TRACKING_ID = os.environ.get("ALI_TRACKING_ID", "").strip()
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY", "").strip()
# ğŸ’ ì‚¬ì´íŠ¸ ì£¼ì†Œ ëì— '/'ê°€ ì—†ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.
SITE_URL = "https://rkskqdl-a11y.github.io/ali-must-buy-items"

ID_LOG_FILE = "posted_ids.txt"

def load_posted_ids():
    if os.path.exists(ID_LOG_FILE):
        with open(ID_LOG_FILE, "r") as f:
            return set(line.strip() for line in f if line.strip())
    return set()

def save_posted_id(p_id):
    with open(ID_LOG_FILE, "a") as f:
        f.write(f"{p_id}\n")

def get_ali_products():
    """ë‹¤ì–‘í•œ ì¹´í…Œê³ ë¦¬ì™€ ì •ë ¬ ë°©ì‹ì„ ëœë¤í•˜ê²Œ ì„ íƒí•˜ì—¬ ìƒí’ˆì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
    cat_ids = ["3", "1501", "34", "66", "7", "44", "502", "1503", "1511", "18", "509", "200000343", "200000345", "200000532", "26", "15", "2", "1524", "21", "13"]
    cat_id = random.choice(cat_ids)
    
    url = "https://api-sg.aliexpress.com/sync"
    params = {
        "app_key": ALI_APP_KEY,
        "timestamp": str(int(time.time() * 1000)),
        "sign_method": "sha256",
        "method": "aliexpress.affiliate.product.query",
        "category_ids": cat_id,
        "page_size": "50",
        "target_currency": "USD",
        "target_language": "EN",
        "tracking_id": ALI_TRACKING_ID
    }
    sort_options = ["VOLUME_DESC", "SALE_PRICE_ASC", "SALE_PRICE_DESC", "LAST_VOLUME_ASC"]
    params["sort"] = random.choice(sort_options)

    # ğŸ’ ì„œëª… ìƒì„± (AliExpress API í•„ìˆ˜ ê·œê²©)
    sorted_params = sorted(params.items())
    base_string = "".join([f"{k}{v}" for k, v in sorted_params])
    sign = hmac.new(ALI_SECRET.encode('utf-8'), base_string.encode('utf-8'), hashlib.sha256).hexdigest().upper()
    params["sign"] = sign
    
    try:
        response = requests.post(url, data=params, timeout=20)
        # ğŸ’ ì•ˆì „í•œ JSON íŒŒì‹±
        res_json = response.json()
        return res_json.get("aliexpress_affiliate_product_query_response", {}).get("resp_result", {}).get("result", {}).get("products", {}).get("product", [])
    except Exception as e:
        print(f"âŒ Ali API Error: {e}")
        return []

def generate_blog_content(product):
    """ğŸ’ 1,000ì ì´ìƒì˜ ì¥ë¬¸ ë¦¬ë·°ë¥¼ ì‘ì„±í•˜ë„ë¡ í”„ë¡¬í”„íŠ¸ë¥¼ ëŒ€í­ ê°•í™”í–ˆìŠµë‹ˆë‹¤."""
    url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={GEMINI_API_KEY}"
    
    title = product.get('product_title')
    price = product.get('target_sale_price')
    
    # ğŸ¤– AIë¥¼ ì••ë°•í•˜ëŠ” êµ¬ì²´ì ì¸ ì¥ë¬¸ ì§€ì‹œì„œ
    prompt = f"""
    Write a detailed professional product review column for: "{title}". 
    The product price is ${price}.
    
    [Requirements]
    1. Language: English
    2. Length: Minimum 1,000 characters.
    3. Style: Expert tech/lifestyle blogger.
    4. Structure: Use the following H3 sections:
       - ### ğŸ” Professional Overview & Design
       - ### ğŸš€ Performance & Real-world Testing
       - ### ğŸ’¡ Why We Recommend This Item
       - ### ğŸ’° Value Analysis & Final Verdict
    5. Formatting: Use Markdown (bold, bullet points) for readability.
    6. Content: Do NOT mention "discount rate" or "sale". Focus on quality and value.
    """
    
    payload = {
        "contents": [{"parts": [{"text": prompt}]}],
        "safetySettings": [{"category": c, "threshold": "BLOCK_NONE"} for c in ["HARM_CATEGORY_HARASSMENT", "HARM_CATEGORY_HATE_SPEECH", "HARM_CATEGORY_SEXUALLY_EXPLICIT", "HARM_CATEGORY_DANGEROUS_CONTENT"]]
    }
    
    try:
        response = requests.post(url, json=payload, timeout=45)
        res_json = response.json()
        
        # ğŸ’ ì•ˆì „í•œ AI ì‘ë‹µ ì¶”ì¶œ (ë¦¬ìŠ¤íŠ¸ ì¸ë±ìŠ¤ [0] í•„ìˆ˜)
        if "candidates" in res_json and len(res_json["candidates"]) > 0:
            candidate = res_json["candidates"][0]
            if "content" in candidate:
                return candidate["content"]["parts"][0]["text"].strip()
        
        if "429" in str(res_json):
            print("â³ Quota limit reached. Resting...")
            time.sleep(70)
    except Exception as e:
        print(f"âŒ Gemini Error: {e}")
    return None

def update_seo_files():
    """ğŸ’ ì‚¬ì´íŠ¸ë§µì˜ ì²« ì¤„ ê³µë°± ë¬¸ì œë¥¼ ì›ì²œ ì°¨ë‹¨í–ˆìŠµë‹ˆë‹¤."""
    posts = sorted([f for f in os.listdir("_posts") if f.endswith(".md")], reverse=True)
    now = datetime.now().strftime("%Y-%m-%d")
    
    # ğŸ’ ì¤‘ìš”: ë¬¸ìì—´ ì‹œì‘ ì‹œ ê³µë°±ì´ë‚˜ ì¤„ë°”ê¿ˆì´ ì ˆëŒ€ ì—†ì–´ì•¼ í•©ë‹ˆë‹¤.
    sitemap_content = '<?xml version="1.0" encoding="UTF-8"?>\n'
    sitemap_content += '<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">\n'
    sitemap_content += f'  <url><loc>{SITE_URL}/</loc><lastmod>{now}</lastmod><priority>1.0</priority></url>\n'
    
    for p in posts:
        # íŒŒì¼ëª… í˜•ì‹: 2026-01-29-12345.md -> URL í˜•ì‹: /2026/01/29/12345.html
        name_parts = p.replace(".md", "").split("-")
        if len(name_parts) >= 4:
            year, month, day = name_parts[0], name_parts[1], name_parts[2]
            title_id = "-".join(name_parts[3:])
            loc_url = f"{SITE_URL}/{year}/{month}/{day}/{title_id}.html"
            sitemap_content += f'  <url><loc>{loc_url}</loc><lastmod>{now}</lastmod></url>\n'
    
    sitemap_content += '</urlset>'
    
    # ğŸ’ íŒŒì¼ ì“°ê¸° (strip()ìœ¼ë¡œ í˜¹ì‹œ ëª¨ë¥¼ ì•ë’¤ ê³µë°± ì œê±°)
    with open("sitemap.xml", "w", encoding="utf-8") as f:
        f.write(sitemap_content.strip())
        
    with open("robots.txt", "w", encoding="utf-8") as f:
        f.write(f"User-agent: *\nAllow: /\nSitemap: {SITE_URL}/sitemap.xml")

def main():
    os.makedirs("_posts", exist_ok=True)
    today_str = datetime.now().strftime("%Y-%m-%d")
    posted_ids = load_posted_ids()
    success_count = 0
    max_posts = 10 
    disclosure = "> **Affiliate Disclosure:** As an AliExpress Associate, I earn from qualifying purchases.\n\n"

    print(f"ğŸš€ Mission Start: Generating {max_posts} Posts for {today_str}")

    while success_count < max_posts:
        products = get_ali_products()
        if not products: 
            print("âš ï¸ No products found. Retrying in 10s...")
            time.sleep(10)
            continue
            
        for p in products:
            if success_count >= max_posts: break
            p_id = str(p.get('product_id'))
            if p_id in posted_ids: continue
            
            img_url = p.get('product_main_image_url', '').strip()
            if img_url.startswith('//'): img_url = 'https:' + img_url
            img_url = img_url.split('?')[0] # ğŸ’ ê¹¨ë—í•œ ì´ë¯¸ì§€ ì£¼ì†Œ ì¶”ì¶œ

            print(f"ğŸ“ Analyzing: {p_id}...")
            content = generate_blog_content(p)
            
            # AI ìƒì„± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ í…Œì´ë¸”ë¡œ ëŒ€ì²´
            if not content:
                content = (
                    "### Product Technical Specs\n\n"
                    "| Attribute | Description |\n"
                    "| :--- | :--- |\n"
                    f"| **Product** | {p.get('product_title')} |\n"
                    f"| **Price** | ${p.get('target_sale_price')} |\n"
                    "| **Evaluation** | Expert Choice |\n"
                )

            # Jekyll í¬ìŠ¤íŠ¸ ìƒì„±
            file_path = f"_posts/{today_str}-{p_id}.md"
            with open(file_path, "w", encoding="utf-8") as f:
                f.write(f"---\nlayout: post\ntitle: \"{p['product_title']}\"\ndate: {today_str}\n---\n\n"
                        f"{disclosure}"
                        f"<img src=\"{img_url}\" alt=\"{p['product_title']}\" referrerpolicy=\"no-referrer\" style=\"width:100%; max-width:600px; display:block; margin:20px 0;\">\n\n"
                        f"{content}\n\n"
                        f"### [ğŸ›’ View Details on AliExpress]({p.get('promotion_link')})")
            
            save_posted_id(p_id)
            posted_ids.add(p_id)
            success_count += 1
            print(f"   âœ… COMPLETED ({success_count}/{max_posts}): {p_id}")
            time.sleep(8) # RPM ì œí•œì„ ê³ ë ¤í•œ ì•ˆì •ì ì¸ ëŒ€ê¸° ì‹œê°„

    update_seo_files()
    print(f"ğŸ Mission Completed & SEO Files Synchronized!")

if __name__ == "__main__":
    main()
